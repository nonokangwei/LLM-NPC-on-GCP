{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A4Tim1m4STn"
      },
      "source": [
        "# Upload Dialog (Pre-defined scripts) to Google Cloud Vector DB\n",
        "\n",
        "Vector Search + Bigtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM5TWeYIBatH"
      },
      "source": [
        "# Install library, remember to restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i_os7y3RBFHN",
        "outputId": "6dbd81e3-1eeb-4a3f-8712-88b12744c5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-aiplatform==1.25.0\n",
            "  Downloading google_cloud_aiplatform-1.25.0-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (3.10.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (1.10.3)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (1.8.5.post1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.56.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.25.0) (0.12.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (0.5.0)\n",
            "Installing collected packages: google-cloud-aiplatform\n",
            "  Attempting uninstall: google-cloud-aiplatform\n",
            "    Found existing installation: google-cloud-aiplatform 1.30.1\n",
            "    Uninstalling google-cloud-aiplatform-1.30.1:\n",
            "      Successfully uninstalled google-cloud-aiplatform-1.30.1\n",
            "Successfully installed google-cloud-aiplatform-1.25.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/74/34/e1a55be448d4fd1866516b9612097c76c7d384d64490a37677c667de58bc/langchain-0.0.297-py3-none-any.whl.metadata\n",
            "  Downloading langchain-0.0.297-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/13/75/82ce74880711ced796fd5a32e4d40c5a32dbea3f1c5e219a8b0544b7bd8c/dataclasses_json-0.6.0-py3-none-any.whl.metadata\n",
            "  Downloading dataclasses_json-0.6.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Obtaining dependency information for langsmith<0.1.0,>=0.0.38 from https://files.pythonhosted.org/packages/fb/58/89739e01b4ef4018efc4a1a107fbba4af8215b59b5bd8caac6cb2910bd7e/langsmith-0.0.39-py3-none-any.whl.metadata\n",
            "  Downloading langsmith-0.0.39-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.1.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading langchain-0.0.297-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.0-py3-none-any.whl (27 kB)\n",
            "Downloading langsmith-0.0.39-py3-none-any.whl (38 kB)\n",
            "Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.0 langchain-0.0.297 langsmith-0.0.39 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting google-cloud-bigtable\n",
            "  Obtaining dependency information for google-cloud-bigtable from https://files.pythonhosted.org/packages/f1/2e/fcdd3aed46825c04bc93c0223fff674059e6a789cfda5374948bea1a3f6e/google_cloud_bigtable-2.21.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable) (2.3.3)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable) (0.12.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (1.56.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigtable) (0.5.0)\n",
            "Downloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.0/293.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-bigtable\n",
            "Successfully installed google-cloud-bigtable-2.21.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install google-cloud-aiplatform==1.25.0\n",
        "! pip install langchain\n",
        "! pip install google-cloud-bigtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Gx7aXVBdvG"
      },
      "source": [
        "# Authenticate to Google Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0SlH-I7BZbS"
      },
      "outputs": [],
      "source": [
        "# Authenticate with Google Cloud credentials\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUMA6IjzBxWr"
      },
      "source": [
        "# Utility functions to create Vector Search\n",
        "\n",
        "use ai_platform_v1 SDK, independent with Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e20_va5GB6LR",
        "outputId": "2c422775-7d0b-41e9-a618-1faf6cac28c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing matching_engine_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile matching_engine_utils.py\n",
        "from datetime import datetime\n",
        "import time\n",
        "import logging\n",
        "\n",
        "from google.cloud import aiplatform_v1 as aipv1\n",
        "from google.protobuf import struct_pb2\n",
        "\n",
        "logging.basicConfig(level = logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "class MatchingEngineUtils:\n",
        "    def __init__(self,\n",
        "                 project_id: str,\n",
        "                 region: str,\n",
        "                 index_name: str):\n",
        "        self.project_id = project_id\n",
        "        self.region = region\n",
        "        self.index_name = index_name\n",
        "        self.index_endpoint_name = f\"{self.index_name}-endpoint\"\n",
        "        self.PARENT = f\"projects/{self.project_id}/locations/{self.region}\"\n",
        "\n",
        "        ENDPOINT = f\"{self.region}-aiplatform.googleapis.com\"\n",
        "        # set index client\n",
        "        self.index_client = aipv1.IndexServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT)\n",
        "        )\n",
        "        # set index endpoint client\n",
        "        self.index_endpoint_client = aipv1.IndexEndpointServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT)\n",
        "        )\n",
        "\n",
        "    def get_index(self):\n",
        "        # Check if index exists\n",
        "        request = aipv1.ListIndexesRequest(parent=self.PARENT)\n",
        "        page_result = self.index_client.list_indexes(request=request)\n",
        "        indexes = [response.name for response in page_result\n",
        "                   if response.display_name == self.index_name]\n",
        "\n",
        "        if len(indexes) == 0:\n",
        "            return None\n",
        "        else:\n",
        "            index_id = indexes[0]\n",
        "            request = aipv1.GetIndexRequest(name=index_id)\n",
        "            index = self.index_client.get_index(request=request)\n",
        "            return index\n",
        "\n",
        "    def get_index_endpoint(self):\n",
        "        # Check if index endpoint exists\n",
        "        request = aipv1.ListIndexEndpointsRequest(parent=self.PARENT)\n",
        "        page_result = self.index_endpoint_client.list_index_endpoints(request=request)\n",
        "        index_endpoints = [response.name for response in page_result\n",
        "                           if response.display_name == self.index_endpoint_name]\n",
        "\n",
        "        if len(index_endpoints) == 0:\n",
        "            return None\n",
        "        else:\n",
        "            index_endpoint_id = index_endpoints[0]\n",
        "            request = aipv1.GetIndexEndpointRequest(name=index_endpoint_id)\n",
        "            index_endpoint = self.index_endpoint_client.get_index_endpoint(request=request)\n",
        "            return index_endpoint\n",
        "\n",
        "    def create_index(self,\n",
        "                     embedding_gcs_uri: str,\n",
        "                     dimensions: int\n",
        "                     ):\n",
        "        # Get index\n",
        "        index = self.get_index()\n",
        "        # Create index if does not exists\n",
        "        if index:\n",
        "            logger.info(f\"Index {self.index_name} already exists with id {index.name}\")\n",
        "        else:\n",
        "            logger.info(f\"Index {self.index_name} does not exists. Creating index ...\")\n",
        "\n",
        "            treeAhConfig = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=500),\n",
        "                    \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
        "                }\n",
        "            )\n",
        "            algorithmConfig = struct_pb2.Struct(\n",
        "                fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
        "            )\n",
        "            config = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"dimensions\": struct_pb2.Value(number_value=dimensions),\n",
        "                    \"approximateNeighborsCount\": struct_pb2.Value(number_value=150),\n",
        "                    \"distanceMeasureType\": struct_pb2.Value(string_value=\"DOT_PRODUCT_DISTANCE\"),\n",
        "                    \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
        "                    \"shardSize\": struct_pb2.Value(string_value=\"SHARD_SIZE_SMALL\"),\n",
        "                }\n",
        "            )\n",
        "            metadata = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"config\": struct_pb2.Value(struct_value=config),\n",
        "                    \"contentsDeltaUri\": struct_pb2.Value(string_value=embedding_gcs_uri),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            index_request = {\n",
        "                \"display_name\": self.index_name,\n",
        "                \"description\": \"Index for LangChain demo\",\n",
        "                \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
        "                \"index_update_method\": aipv1.Index.IndexUpdateMethod.STREAM_UPDATE,\n",
        "            }\n",
        "\n",
        "            r = self.index_client.create_index(parent=self.PARENT,\n",
        "                                               index=index_request)\n",
        "\n",
        "            # Poll the operation until it's done successfullly.\n",
        "            logging.info(\"Poll the operation to create index ...\")\n",
        "            while True:\n",
        "                if r.done():\n",
        "                    break\n",
        "                time.sleep(60)\n",
        "                print('.', end='')\n",
        "\n",
        "            index = r.result()\n",
        "            logger.info(f\"Index {self.index_name} created with resource name as {index.name}\")\n",
        "\n",
        "        return index\n",
        "\n",
        "    def deploy_index(self,\n",
        "                     machine_type: str = \"e2-standard-2\",\n",
        "                     min_replica_count: int = 2,\n",
        "                     max_replica_count: int = 10,\n",
        "                     network: str = None):\n",
        "        try:\n",
        "            # Get index if exists\n",
        "            index = self.get_index()\n",
        "            if not index:\n",
        "                raise Exception(f\"Index {self.index_name} does not exists. Please create index before deploying.\")\n",
        "\n",
        "            # Get index endpoint if exists\n",
        "            index_endpoint = self.get_index_endpoint()\n",
        "            # Create Index Endpoint if does not exists\n",
        "            if index_endpoint:\n",
        "                logger.info(f\"Index endpoint {self.index_endpoint_name} already exists with resource \" +\n",
        "                            f\"name as {index_endpoint.name} and endpoint domain name as \" +\n",
        "                            f\"{index_endpoint.public_endpoint_domain_name}\")\n",
        "            else:\n",
        "                logger.info(f\"Index endpoint {self.index_endpoint_name} does not exists. Creating index endpoint...\")\n",
        "                index_endpoint_request = {\"display_name\": self.index_endpoint_name}\n",
        "\n",
        "                if network:\n",
        "                    index_endpoint_request[\"network\"] = network\n",
        "                else:\n",
        "                    index_endpoint_request[\"public_endpoint_enabled\"] = True\n",
        "\n",
        "                r = self.index_endpoint_client.create_index_endpoint(\n",
        "                    parent=self.PARENT,\n",
        "                    index_endpoint=index_endpoint_request)\n",
        "\n",
        "                logger.info(\"Poll the operation to create index endpoint ...\")\n",
        "                while True:\n",
        "                    if r.done():\n",
        "                        break\n",
        "                    time.sleep(60)\n",
        "                    print('.', end='')\n",
        "\n",
        "                index_endpoint = r.result()\n",
        "                logger.info(f\"Index endpoint {self.index_endpoint_name} created with resource \" +\n",
        "                            f\"name as {index_endpoint.name} and endpoint domain name as \" +\n",
        "                            f\"{index_endpoint.public_endpoint_domain_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create index endpoint {self.index_endpoint_name}\")\n",
        "            raise e\n",
        "\n",
        "        # Deploy Index to endpoint\n",
        "        try:\n",
        "            # Check if index is already deployed to the endpoint\n",
        "            for d_index in index_endpoint.deployed_indexes:\n",
        "                if d_index.index == index.name:\n",
        "                    logger.info(f\"Skipping deploying Index. Index {self.index_name}\" +\n",
        "                                f\"already deployed with id {index.name} to the index endpoint {self.index_endpoint_name}\")\n",
        "                    return index_endpoint\n",
        "\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "            deployed_index_id = f\"{self.index_name.replace('-', '_')}_{timestamp}\"\n",
        "            deploy_index = {\n",
        "                \"id\": deployed_index_id,\n",
        "                \"display_name\": deployed_index_id,\n",
        "                \"index\": index.name,\n",
        "                \"dedicated_resources\": {\n",
        "                    \"machine_spec\": {\n",
        "                        \"machine_type\": machine_type,\n",
        "                        },\n",
        "                    \"min_replica_count\": min_replica_count,\n",
        "                    \"max_replica_count\": max_replica_count\n",
        "                    }\n",
        "            }\n",
        "            logger.info(f\"Deploying index with request = {deploy_index}\")\n",
        "            r = self.index_endpoint_client.deploy_index(\n",
        "                index_endpoint=index_endpoint.name,\n",
        "                deployed_index=deploy_index\n",
        "            )\n",
        "\n",
        "            # Poll the operation until it's done successfullly.\n",
        "            logger.info(\"Poll the operation to deploy index ...\")\n",
        "            while True:\n",
        "                if r.done():\n",
        "                    break\n",
        "                time.sleep(60)\n",
        "                print('.', end='')\n",
        "\n",
        "            logger.info(f\"Deployed index {self.index_name} to endpoint {self.index_endpoint_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to deploy index {self.index_name} to the index endpoint {self.index_endpoint_name}\")\n",
        "            raise e\n",
        "\n",
        "        return index_endpoint\n",
        "\n",
        "    def get_index_and_endpoint(self):\n",
        "        # Get index id if exists\n",
        "        index = self.get_index()\n",
        "        index_id = index.name if index else ''\n",
        "\n",
        "        # Get index endpoint id if exists\n",
        "        index_endpoint = self.get_index_endpoint()\n",
        "        index_endpoint_id = index_endpoint.name if index_endpoint else ''\n",
        "\n",
        "        return index_id, index_endpoint_id\n",
        "\n",
        "    def delete_index(self):\n",
        "        # Check if index exists\n",
        "        index = self.get_index()\n",
        "\n",
        "        # create index if does not exists\n",
        "        if index:\n",
        "            # Delete index\n",
        "            index_id = index.name\n",
        "            logger.info(f\"Deleting Index {self.index_name} with id {index_id}\")\n",
        "            self.index_client.delete_index(name=index_id)\n",
        "        else:\n",
        "            raise Exception(\"Index {index_name} does not exists.\")\n",
        "\n",
        "    def delete_index_endpoint(self):\n",
        "        # Check if index endpoint exists\n",
        "        index_endpoint = self.get_index_endpoint()\n",
        "\n",
        "        # Create Index Endpoint if does not exists\n",
        "        if index_endpoint:\n",
        "            logger.info(f\"Index endpoint {self.index_endpoint_name}  exists with resource \" +\n",
        "                        f\"name as {index_endpoint.name} and endpoint domain name as \" +\n",
        "                        f\"{index_endpoint.public_endpoint_domain_name}\")\n",
        "\n",
        "            index_endpoint_id = index_endpoint.name\n",
        "            index_endpoint = self.index_endpoint_client.get_index_endpoint(name=index_endpoint_id)\n",
        "            # Undeploy existing indexes\n",
        "            for d_index in index_endpoint.deployed_indexes:\n",
        "                logger.info(f\"Undeploying index with id {d_index.id} from Index endpoint {self.index_endpoint_name}\")\n",
        "                request = aipv1.UndeployIndexRequest(\n",
        "                    index_endpoint=index_endpoint_id,\n",
        "                    deployed_index_id=d_index.id)\n",
        "                r = self.index_endpoint_client.undeploy_index(request=request)\n",
        "                response = r.result()\n",
        "                logger.info(response)\n",
        "\n",
        "            # Delete index endpoint\n",
        "            logger.info(f\"Deleting Index endpoint {self.index_endpoint_name} with id {index_endpoint_id}\")\n",
        "            self.index_endpoint_client.delete_index_endpoint(name=index_endpoint_id)\n",
        "        else:\n",
        "            raise Exception(f\"Index endpoint {self.index_endpoint_name} does not exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-AU4IOwCXky"
      },
      "source": [
        "# Create Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhw6HuceC3YA"
      },
      "outputs": [],
      "source": [
        "# @title parameters\n",
        "PROJECT_ID = \"argolis-lsj-test\" # @param {type: \"string\"}\n",
        "LOCATION = \"us-central1\"\n",
        "ME_REGION          = \"us-central1\"\n",
        "ME_INDEX_NAME      = \"langchain-vs-tencent-colabEnterprise\" # @param {type: \"string\"}\n",
        "ME_DIMENSIONS      = 768 # when using Vertex PaLM Embedding\n",
        "ME_EMBEDDING_DIR   = \"gs://langchain-vs-tencent-bucket\" # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBRR5IdzRU8n",
        "outputId": "369cbe21-51e4-483c-8a93-170abe951af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating gs://langchain-vs-tencent-bucket/...\n"
          ]
        }
      ],
      "source": [
        "# @title create a gcs bucket for vector search initialization\n",
        "! gsutil mb -l $LOCATION -p $PROJECT_ID $ME_EMBEDDING_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiPVaAc9CDdS",
        "outputId": "9421b2f2-9e93-48eb-972f-aa53ac0a39cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://embeddings_0.json [Content-Type=application/json]...\n",
            "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
            "Operation completed over 1 objects/3.8 KiB.                                      \n"
          ]
        }
      ],
      "source": [
        "# @title init matching engine\n",
        "from google.cloud import storage\n",
        "import uuid\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "# dummy embedding\n",
        "init_embedding = {\"id\": str(uuid.uuid4()),\n",
        "                  \"embedding\": list(np.zeros(ME_DIMENSIONS))}\n",
        "\n",
        "# dump embedding to a local file\n",
        "with open(\"embeddings_0.json\", \"w\") as f:\n",
        "    json.dump(init_embedding, f)\n",
        "\n",
        "# write embedding to Cloud Storage\n",
        "! gsutil cp embeddings_0.json {ME_EMBEDDING_DIR}/init_index/embeddings_0.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHJm9sebCyP3",
        "outputId": "a8bcc648-d4cf-4f78-f32a-81175f4f197a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".........................................................projects/703099487153/locations/us-central1/indexes/7197024883421741056\n"
          ]
        }
      ],
      "source": [
        "# @title create index, wait for 30 mins\n",
        "import matching_engine_utils\n",
        "mengine = matching_engine_utils.MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)\n",
        "index = mengine.create_index(f\"{ME_EMBEDDING_DIR}/init_index\", ME_DIMENSIONS)\n",
        "if index:\n",
        "  print(index.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpWBzUltD4Vr",
        "outputId": "5abf9898-d719-49af-8da6-5c4c787fbffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "....................................Index endpoint resource name: projects/703099487153/locations/us-central1/indexEndpoints/2049410509337264128\n",
            "Index endpoint public domain name: \n",
            "Deployed indexes on the index endpoint:\n"
          ]
        }
      ],
      "source": [
        "# @title deploy index to endpoint, wait for 30 mins\n",
        "#import matching_engine_utils\n",
        "#mengine = matching_engine_utils.MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)\n",
        "index_endpoint = mengine.deploy_index(machine_type=\"n1-standard-16\", max_replica_count=2)\n",
        "if index_endpoint:\n",
        "  print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
        "  print(f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\")\n",
        "  print(f\"Deployed indexes on the index endpoint:\")\n",
        "  for d in index_endpoint.deployed_indexes:\n",
        "    print(f\"    {d.id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbQ9pVsCEQHF"
      },
      "source": [
        "# Vector Search implementation of the Vector Store in Langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkAdiSU3EKeb",
        "outputId": "1a5780b9-89a1-422e-ea5a-e511f33cf0fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing matching_engine_bigtable.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile matching_engine_bigtable.py\n",
        "\"\"\"Vertex Matching Engine implementation of the vector store.\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import uuid, hashlib\n",
        "from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Type\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import TensorflowHubEmbeddings\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.cloud import bigtable\n",
        "import google.cloud.bigtable.row_filters as row_filters\n",
        "from google.cloud import aiplatform_v1beta1\n",
        "from google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint\n",
        "from google.cloud.aiplatform_v1beta1 import FindNeighborsResponse\n",
        "from google.oauth2.service_account import Credentials\n",
        "from google.cloud import aiplatform_v1\n",
        "\n",
        "logger = logging.getLogger()\n",
        "bigtable_table_id = \"vectordb\"\n",
        "bigtable_column_family_id = \"vector_text\"\n",
        "collection_list = [\"default\"]\n",
        "ENDPOINT = \"{}-aiplatform.googleapis.com\".format(\"us-central1\")\n",
        "\n",
        "class MatchingEngine(VectorStore):\n",
        "    \"\"\"Vertex Matching Engine implementation of the vector store.\n",
        "\n",
        "    While the embeddings are stored in the Matching Engine, the embedded\n",
        "    documents will be stored in GCS.\n",
        "\n",
        "    An existing Index and corresponding Endpoint are preconditions for\n",
        "    using this module.\n",
        "\n",
        "    See usage in docs/modules/indexes/vectorstores/examples/matchingengine.ipynb\n",
        "\n",
        "    Note that this implementation is mostly meant for reading if you are\n",
        "    planning to do a real time implementation. While reading is a real time\n",
        "    operation, updating the index takes close to one hour.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        project_id: str,\n",
        "        index: MatchingEngineIndex,\n",
        "        endpoint: MatchingEngineIndexEndpoint,\n",
        "        embedding: Embeddings,\n",
        "        gcs_client: storage.Client,\n",
        "        gcs_bucket_name: str,\n",
        "        index_client: aiplatform_v1.IndexServiceClient,\n",
        "        index_endpoint_client: aiplatform_v1.IndexEndpointServiceClient,\n",
        "        bigtable_client: bigtable.Client,\n",
        "        bigtable_instance_id: str,\n",
        "        credentials: Optional[Credentials] = None,\n",
        "    ):\n",
        "        \"\"\"Vertex Matching Engine implementation of the vector store.\n",
        "\n",
        "        While the embeddings are stored in the Matching Engine, the embedded\n",
        "        documents will be stored in GCS.\n",
        "\n",
        "        An existing Index and corresponding Endpoint are preconditions for\n",
        "        using this module.\n",
        "\n",
        "        See usage in\n",
        "        docs/modules/indexes/vectorstores/examples/matchingengine.ipynb.\n",
        "\n",
        "        Note that this implementation is mostly meant for reading if you are\n",
        "        planning to do a real time implementation. While reading is a real time\n",
        "        operation, updating the index takes close to one hour.\n",
        "\n",
        "        Attributes:\n",
        "            project_id: The GCS project id.\n",
        "            index: The created index class. See\n",
        "                ~:func:`MatchingEngine.from_components`.\n",
        "            endpoint: The created endpoint class. See\n",
        "                ~:func:`MatchingEngine.from_components`.\n",
        "            embedding: A :class:`Embeddings` that will be used for\n",
        "                embedding the text sent. If none is sent, then the\n",
        "                multilingual Tensorflow Universal Sentence Encoder will be used.\n",
        "            gcs_client: The GCS client.\n",
        "            gcs_bucket_name: The GCS bucket name.\n",
        "            credentials (Optional): Created GCP credentials.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._validate_google_libraries_installation()\n",
        "\n",
        "        self.project_id = project_id\n",
        "        self.index = index\n",
        "        self.endpoint = endpoint\n",
        "        self.embedding = embedding\n",
        "        self.bigtable_client = bigtable_client\n",
        "        self.credentials = credentials\n",
        "        self.bigtable_instance_id = bigtable_instance_id\n",
        "        self.gcs_client = gcs_client\n",
        "        self.gcs_bucket_name = gcs_bucket_name\n",
        "        self.index_client = index_client\n",
        "        self.index_endpoint_client = index_endpoint_client\n",
        "\n",
        "    def _validate_google_libraries_installation(self) -> None:\n",
        "        \"\"\"Validates that Google libraries that are needed are installed.\"\"\"\n",
        "        try:\n",
        "            from google.cloud import aiplatform, bigtable, storage  # noqa: F401\n",
        "            from google.oauth2 import service_account  # noqa: F401\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                \"You must run `pip install --upgrade \"\n",
        "                \"google-cloud-aiplatform google-cloud-storage google-cloud-bigtable`\"\n",
        "                \"to use the MatchingEngine Vectorstore.\"\n",
        "            )\n",
        "\n",
        "    def list_collection(\n",
        "        self,\n",
        "        **kwargs: Any,\n",
        "    )-> List[str]:\n",
        "        return collection_list\n",
        "\n",
        "    def add_texts(\n",
        "        self,\n",
        "        texts: Iterable[str],\n",
        "        collection: str,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
        "\n",
        "        Args:\n",
        "            texts: Iterable of strings to add to the vectorstore.\n",
        "            metadatas: Optional list of metadatas associated with the texts.\n",
        "            kwargs: vectorstore specific parameters.\n",
        "\n",
        "        Returns:\n",
        "            List of ids from adding the texts into the vectorstore.\n",
        "        \"\"\"\n",
        "        logger.debug(\"Embedding documents.\")\n",
        "        embeddings = self.embedding.embed_documents(texts=list(texts), batch_size=5)\n",
        "        jsons = []\n",
        "        ids = []\n",
        "        global collection_list\n",
        "\n",
        "        if collection in collection_list:\n",
        "            pass\n",
        "        else:\n",
        "            collection_list.append(collection)\n",
        "\n",
        "        # Could be improved with async.\n",
        "        for embedding, text in zip(embeddings, texts):\n",
        "            # id = str(uuid.uuid4())\n",
        "            id = hashlib.sha256((str(text)+str(collection)).encode('utf-8')).hexdigest()\n",
        "            ids.append(id)\n",
        "            jsons.append({\"id\": id, \"embedding\": embedding, \"text\": text})\n",
        "            # self._upload_to_gcs(text, f\"documents/{id}\")\n",
        "\n",
        "        # logger.debug(f\"Uploaded {len(ids)} documents to GCS.\")\n",
        "\n",
        "        # # Creating json lines from the embedded documents.\n",
        "        # result_str = \"\\n\".join([json.dumps(x) for x in jsons])\n",
        "\n",
        "        # filename_prefix = f\"indexes/{uuid.uuid4()}\"\n",
        "        # filename = f\"{filename_prefix}/{time.time()}.json\"\n",
        "        # self._upload_to_gcs(result_str, filename)\n",
        "        # logger.debug(\n",
        "        #     f\"Uploaded updated json with embeddings to \"\n",
        "        #     f\"{self.gcs_bucket_name}/{filename}.\"\n",
        "        # )\n",
        "\n",
        "        # self.index = self.index.update_embeddings(\n",
        "        #     contents_delta_uri=f\"gs://{self.gcs_bucket_name}/{filename_prefix}/\"\n",
        "        # )\n",
        "        self._stream_upsert_vme(data=jsons, collection=collection)\n",
        "        logger.debug(\"Updated index with new configuration.\")\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def delete_texts(\n",
        "        self,\n",
        "        texts: Iterable[str],\n",
        "        collection: str,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Run more texts through the embeddings and delete from the vectorstore.\n",
        "\n",
        "        Args:\n",
        "            texts: Iterable of strings to delete from the vectorstore.\n",
        "            metadatas: Optional list of metadatas associated with the texts.\n",
        "            kwargs: vectorstore specific parameters.\n",
        "\n",
        "        Returns:\n",
        "            List of ids from deleting the texts into the vectorstore.\n",
        "        \"\"\"\n",
        "        logger.debug(\"Delete embedding documents.\")\n",
        "        ids = []\n",
        "        jsons =[]\n",
        "        # Could be improved with async.\n",
        "\n",
        "        for text in texts:\n",
        "            # id = str(uuid.uuid4())\n",
        "            id = hashlib.sha256((str(text)+str(collection)).encode('utf-8')).hexdigest()\n",
        "            ids.append(id)\n",
        "            jsons.append({\"id\": id})\n",
        "\n",
        "        self._stream_delete_vme(data=jsons, collection=collection)\n",
        "        logger.debug(\"Delete index with new configuration.\")\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def delete_collection(\n",
        "        self,\n",
        "        collection: str,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> List[str]:\n",
        "        logger.debug(\"Delete embedding documents by collection\")\n",
        "        ids = []\n",
        "        jsons = []\n",
        "\n",
        "        instance_client = self.bigtable_client.instance(self.bigtable_instance_id)\n",
        "        table_client = instance_client.table(bigtable_table_id)\n",
        "        collection_name = collection\n",
        "\n",
        "        try:\n",
        "            rows = table_client.read_rows(\n",
        "                filter_=row_filters.ValueRegexFilter(collection_name.encode(\"utf-8\"))\n",
        "            )\n",
        "        except Exception as error:\n",
        "            logger.debug(f\"Fail to fetech collection data {collection_name} with error {error}\")\n",
        "\n",
        "        for row in rows:\n",
        "            id = row.row_key.decode(\"utf-8\")\n",
        "            ids.append(id)\n",
        "            jsons.append({\"id\": id})\n",
        "\n",
        "        if jsons.__len__() > 0:\n",
        "            self._stream_delete_vme(data=jsons, collection=collection_name)\n",
        "            logger.debug(\"Delete index with new configuration.\")\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def _read_text(self, row_key_text: str) -> str:\n",
        "\n",
        "        instance_client = self.bigtable_client.instance(self.bigtable_instance_id)\n",
        "        table_client = instance_client.table(bigtable_table_id)\n",
        "\n",
        "        row_key = row_key_text\n",
        "        row = table_client.read_row(row_key)\n",
        "        column_family_id = bigtable_column_family_id\n",
        "        buf = row.cell_value(column_family_id=column_family_id, column=\"text\".encode('utf-8'))\n",
        "\n",
        "        if buf != None:\n",
        "            return buf.decode(\"utf-8\")\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def _upload_to_gcs(self, data: str, gcs_location: str) -> None:\n",
        "        \"\"\"Uploads data to gcs_location.\n",
        "\n",
        "        Args:\n",
        "            data: The data that will be stored.\n",
        "            gcs_location: The location where the data will be stored.\n",
        "        \"\"\"\n",
        "        bucket = self.gcs_client.get_bucket(self.gcs_bucket_name)\n",
        "        blob = bucket.blob(gcs_location)\n",
        "        blob.upload_from_string(data)\n",
        "\n",
        "    def _stream_upsert_vme(self, data: List[dict], collection: str) -> None:\n",
        "        \"\"\"Stream upsert data to vertex matching engine\n",
        "\n",
        "        Args:\n",
        "            data: The data will be stored [{\"id\": id, \"embedding\": embedding}]\n",
        "        \"\"\"\n",
        "        from google.cloud import aiplatform_v1\n",
        "        import datetime\n",
        "\n",
        "        instance_client = self.bigtable_client.instance(self.bigtable_instance_id)\n",
        "        table_client = instance_client.table(bigtable_table_id)\n",
        "        column_family_id = bigtable_column_family_id\n",
        "\n",
        "        # collection_name = collection\n",
        "\n",
        "        if collection != \"\":\n",
        "            restriction  = aiplatform_v1.IndexDatapoint.Restriction(\n",
        "                {\n",
        "                    \"namespace\" : \"collection\",\n",
        "                    \"allow_list\" : [collection]\n",
        "                }\n",
        "            )\n",
        "            collection_name = collection\n",
        "        else:\n",
        "            restriction  = aiplatform_v1.IndexDatapoint.Restriction({})\n",
        "            collection_name = \"default\"\n",
        "\n",
        "        for embedding_data in data:\n",
        "            datapoint = aiplatform_v1.IndexDatapoint(\n",
        "                datapoint_id=embedding_data[\"id\"],\n",
        "                feature_vector=embedding_data[\"embedding\"],\n",
        "                restricts=[restriction]\n",
        "            )\n",
        "\n",
        "            upsert_datapoint_request = aiplatform_v1.UpsertDatapointsRequest(\n",
        "                        index=self.index.name,\n",
        "                        datapoints=[datapoint]\n",
        "                    )\n",
        "\n",
        "            response = self.index_client.upsert_datapoints(request=upsert_datapoint_request)\n",
        "            if response != {}:\n",
        "                logger.debug(f\"Fail to upsert {embedding_data['id']} embedding data.\")\n",
        "\n",
        "            try:\n",
        "                timestamp = datetime.datetime.utcnow()\n",
        "                row_key = embedding_data[\"id\"]\n",
        "                row = table_client.direct_row(row_key)\n",
        "                row.set_cell(column_family_id, \"text\".encode(\"utf-8\"), embedding_data[\"text\"].encode(\"utf-8\"), timestamp)\n",
        "                row.set_cell(column_family_id,\"collection\".encode(\"utf-8\"), collection_name, timestamp)\n",
        "                row.commit()\n",
        "            except Exception as error:\n",
        "                print(error)\n",
        "                logger.debug(f\"Fail to upsert {embedding_data['id']} with error {error}\")\n",
        "\n",
        "    def _stream_delete_vme(self, data: List[str], collection: str) -> None:\n",
        "        \"\"\"Stream upsert data to vertex matching engine\n",
        "\n",
        "        Args:\n",
        "            data: The data will be stored [{\"id\": id}]\n",
        "        \"\"\"\n",
        "\n",
        "        instance_client = self.bigtable_client.instance(self.bigtable_instance_id)\n",
        "        table_client = instance_client.table(bigtable_table_id)\n",
        "\n",
        "        for id in data:\n",
        "            delete_datapoint_request = aiplatform_v1.RemoveDatapointsRequest(\n",
        "               index=self.index.name,\n",
        "               datapoint_ids=[id['id']]\n",
        "            )\n",
        "\n",
        "            response = self.index_client.remove_datapoints(request=delete_datapoint_request)\n",
        "            if response != None:\n",
        "                logger.debug(f\"Fail to delete {id['id']} embedding data.\")\n",
        "\n",
        "            try:\n",
        "                row_key = id['id']\n",
        "                row = table_client.row(row_key)\n",
        "                row.delete()\n",
        "                row.commit()\n",
        "            except Exception as error:\n",
        "                print(error)\n",
        "                logger.debug(f\"Fail to delete {id['id']} with error {error}\")\n",
        "\n",
        "        global collection_list\n",
        "\n",
        "        if collection != \"\":\n",
        "            collection_name = collection\n",
        "        else:\n",
        "            collection_name = \"default\"\n",
        "\n",
        "        rows = table_client.read_rows(\n",
        "            filter_=row_filters.RowFilterChain(\n",
        "                filters=[\n",
        "                    row_filters.ColumnQualifierRegexFilter(\"collection\".encode(\"utf-8\")),\n",
        "                    row_filters.ValueRegexFilter(collection_name.encode(\"utf-8\")),\n",
        "                    row_filters.CellsColumnLimitFilter(1),\n",
        "                    row_filters.StripValueTransformerFilter(True),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "        rows.consume_all()\n",
        "        if len(rows.rows) == 0:\n",
        "            collection_list.remove(collection_name)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def get_matches(\n",
        "            self,\n",
        "            embeddings: List[str],\n",
        "            n_matches: int,\n",
        "            collection: str) -> FindNeighborsResponse:\n",
        "        '''\n",
        "        get matches from matching engine given a vector query\n",
        "        Uses public endpoint\n",
        "\n",
        "        '''\n",
        "\n",
        "        client_options = {\n",
        "              \"api_endpoint\": self.endpoint.public_endpoint_domain_name\n",
        "        }\n",
        "\n",
        "        vertex_ai_client = aiplatform_v1beta1.MatchServiceClient(\n",
        "              client_options=client_options,\n",
        "        )\n",
        "\n",
        "        request = aiplatform_v1beta1.FindNeighborsRequest(\n",
        "              index_endpoint=self.endpoint.resource_name,\n",
        "              deployed_index_id=self.endpoint.deployed_indexes[0].id,\n",
        "          )\n",
        "\n",
        "        if collection != \"\":\n",
        "            restriction  = aiplatform_v1beta1.IndexDatapoint.Restriction(\n",
        "                {\n",
        "                    \"namespace\" : \"collection\",\n",
        "                    \"allow_list\" : [collection]\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            restriction  =  aiplatform_v1beta1.IndexDatapoint.Restriction(\n",
        "                {}\n",
        "            )\n",
        "\n",
        "        for i, embedding in enumerate(embeddings):\n",
        "            query = aiplatform_v1beta1.FindNeighborsRequest.Query(\n",
        "                datapoint=aiplatform_v1beta1.IndexDatapoint(\n",
        "                        datapoint_id=str(i),\n",
        "                        feature_vector=embedding,\n",
        "                        restricts=[restriction],\n",
        "                    ),\n",
        "                neighbor_count = n_matches\n",
        "            )\n",
        "            request.queries.append(query)\n",
        "\n",
        "        response = vertex_ai_client.find_neighbors(request)\n",
        "        return response\n",
        "\n",
        "    def similarity_search(\n",
        "        self, query: str, k: int = 4, collection: str = \"\", **kwargs: Any\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"Return docs most similar to query.\n",
        "\n",
        "        Args:\n",
        "            query: The string that will be used to search for similar documents.\n",
        "            k: The amount of neighbors that will be retrieved.\n",
        "\n",
        "        Returns:\n",
        "            A list of k matching documents.\n",
        "        \"\"\"\n",
        "\n",
        "        logger.debug(f\"Embedding query {query}.\")\n",
        "        embedding_query = self.embedding.embed_documents([query])\n",
        "\n",
        "        # TO-DO: Pending query sdk integration\n",
        "        # response = self.endpoint.match(\n",
        "        #     deployed_index_id=self._get_index_id(),\n",
        "        #     queries=embedding_query,\n",
        "        #     num_neighbors=k,\n",
        "        # )\n",
        "\n",
        "        response = self.get_matches(embedding_query,\n",
        "                                    k, collection)\n",
        "\n",
        "        if response != None:\n",
        "            response = response.nearest_neighbors[0].neighbors\n",
        "        else:\n",
        "            raise Exception(f\"Failed to query index {str(response)}\")\n",
        "\n",
        "        if len(response) == 0:\n",
        "            return []\n",
        "\n",
        "        logger.debug(f\"Found {len(response)} matches for the query {query}.\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # I'm only getting the first one because queries receives an array\n",
        "        # and the similarity_search method only recevies one query. This\n",
        "        # means that the match method will always return an array with only\n",
        "        # one element.\n",
        "        # for doc in response[0]:\n",
        "        #     page_content = self._download_from_gcs(f\"documents/{doc.id}\")\n",
        "        #     results.append(Document(page_content=page_content))\n",
        "        for doc in response:\n",
        "            page_content = self._read_text(doc.datapoint.datapoint_id)\n",
        "            results.append(Document(page_content=page_content))\n",
        "\n",
        "\n",
        "        logger.debug(\"Downloaded documents for query.\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _get_index_id(self) -> str:\n",
        "        \"\"\"Gets the correct index id for the endpoint.\n",
        "\n",
        "        Returns:\n",
        "            The index id if found (which should be found) or throws\n",
        "            ValueError otherwise.\n",
        "        \"\"\"\n",
        "        for index in self.endpoint.deployed_indexes:\n",
        "            if index.index == self.index.resource_name:\n",
        "                return index.id\n",
        "\n",
        "        raise ValueError(\n",
        "            f\"No index with id {self.index.resource_name} \"\n",
        "            f\"deployed on endpoint \"\n",
        "            f\"{self.endpoint.display_name}.\"\n",
        "        )\n",
        "\n",
        "    def _download_from_gcs(self, gcs_location: str) -> str:\n",
        "        \"\"\"Downloads from GCS in text format.\n",
        "\n",
        "        Args:\n",
        "            gcs_location: The location where the file is located.\n",
        "\n",
        "        Returns:\n",
        "            The string contents of the file.\n",
        "        \"\"\"\n",
        "        bucket = self.gcs_client.get_bucket(self.gcs_bucket_name)\n",
        "        blob = bucket.blob(gcs_location)\n",
        "        return blob.download_as_string()\n",
        "\n",
        "    @classmethod\n",
        "    def from_texts(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        texts: List[str],\n",
        "        embedding: Embeddings,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Use from components instead.\"\"\"\n",
        "        raise NotImplementedError(\n",
        "            \"This method is not implemented. Instead, you should initialize the class\"\n",
        "            \" with `MatchingEngine.from_components(...)` and then call \"\n",
        "            \"`add_texts`\"\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_components(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        gcs_bucket_name: str,\n",
        "        index_id: str,\n",
        "        endpoint_id: str,\n",
        "        instance_id: str,\n",
        "        credentials_path: Optional[str] = None,\n",
        "        embedding: Optional[Embeddings] = None,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Takes the object creation out of the constructor.\n",
        "\n",
        "        Args:\n",
        "            project_id: The GCP project id.\n",
        "            region: The default location making the API calls. It must have\n",
        "            the same location as the GCS bucket and must be regional.\n",
        "            gcs_bucket_name: The location where the vectors will be stored in\n",
        "            order for the index to be created.\n",
        "            index_id: The id of the created index.\n",
        "            endpoint_id: The id of the created endpoint.\n",
        "            credentials_path: (Optional) The path of the Google credentials on\n",
        "            the local file system.\n",
        "            embedding: The :class:`Embeddings` that will be used for\n",
        "            embedding the texts.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngine with the texts added to the index.\n",
        "        \"\"\"\n",
        "        gcs_bucket_name = cls._validate_gcs_bucket(gcs_bucket_name)\n",
        "        credentials = cls._create_credentials_from_file(credentials_path)\n",
        "        index = cls._create_index_by_id(index_id, project_id, region, credentials)\n",
        "        endpoint = cls._create_endpoint_by_id(\n",
        "            endpoint_id, project_id, region, credentials\n",
        "        )\n",
        "\n",
        "        gcs_client = cls._get_gcs_client(credentials, project_id)\n",
        "        bigtable_client = cls._get_bigtable_client(credentials, project_id)\n",
        "        index_client = cls._get_index_client(project_id, region, credentials)\n",
        "        index_endpoint_client = cls._get_index_endpoint_client(project_id, region, credentials)\n",
        "        cls._init_aiplatform(project_id, region, gcs_bucket_name, credentials)\n",
        "\n",
        "        return cls(\n",
        "            project_id=project_id,\n",
        "            index=index,\n",
        "            endpoint=endpoint,\n",
        "            embedding=embedding or cls._get_default_embeddings(),\n",
        "            gcs_client=gcs_client,\n",
        "            index_client=index_client,\n",
        "            index_endpoint_client=index_endpoint_client,\n",
        "            credentials=credentials,\n",
        "            gcs_bucket_name=gcs_bucket_name,\n",
        "            bigtable_client=bigtable_client,\n",
        "            bigtable_instance_id=instance_id\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _validate_gcs_bucket(cls, gcs_bucket_name: str) -> str:\n",
        "        \"\"\"Validates the gcs_bucket_name as a bucket name.\n",
        "\n",
        "        Args:\n",
        "              gcs_bucket_name: The received bucket uri.\n",
        "\n",
        "        Returns:\n",
        "              A valid gcs_bucket_name or throws ValueError if full path is\n",
        "              provided.\n",
        "        \"\"\"\n",
        "        gcs_bucket_name = gcs_bucket_name.replace(\"gs://\", \"\")\n",
        "        if \"/\" in gcs_bucket_name:\n",
        "            raise ValueError(\n",
        "                f\"The argument gcs_bucket_name should only be \"\n",
        "                f\"the bucket name. Received {gcs_bucket_name}\"\n",
        "            )\n",
        "        return gcs_bucket_name\n",
        "\n",
        "    @classmethod\n",
        "    def _create_credentials_from_file(\n",
        "        cls, json_credentials_path: Optional[str]\n",
        "    ) -> Optional[Credentials]:\n",
        "        \"\"\"Creates credentials for GCP.\n",
        "\n",
        "        Args:\n",
        "             json_credentials_path: The path on the file system where the\n",
        "             credentials are stored.\n",
        "\n",
        "         Returns:\n",
        "             An optional of Credentials or None, in which case the default\n",
        "             will be used.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.oauth2 import service_account\n",
        "\n",
        "        credentials = None\n",
        "        if json_credentials_path is not None:\n",
        "            credentials = service_account.Credentials.from_service_account_file(\n",
        "                json_credentials_path\n",
        "            )\n",
        "\n",
        "        return credentials\n",
        "\n",
        "    @classmethod\n",
        "    def _create_index_by_id(\n",
        "        cls, index_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> MatchingEngineIndex:\n",
        "        \"\"\"Creates a MatchingEngineIndex object by id.\n",
        "\n",
        "        Args:\n",
        "            index_id: The created index id.\n",
        "            project_id: The project to retrieve index from.\n",
        "            region: Location to retrieve index from.\n",
        "            credentials: GCS credentials.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngineIndex.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "\n",
        "        logger.debug(f\"Creating matching engine index with id {index_id}.\")\n",
        "        index_client = cls._get_index_client(project_id, region, credentials)\n",
        "        request = aiplatform_v1.GetIndexRequest(name=index_id)\n",
        "        return index_client.get_index(request=request)\n",
        "\n",
        "    @classmethod\n",
        "    def _create_endpoint_by_id(\n",
        "        cls, endpoint_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> MatchingEngineIndexEndpoint:\n",
        "        \"\"\"Creates a MatchingEngineIndexEndpoint object by id.\n",
        "\n",
        "        Args:\n",
        "            endpoint_id: The created endpoint id.\n",
        "            project_id: The project to retrieve index from.\n",
        "            region: Location to retrieve index from.\n",
        "            credentials: GCS credentials.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngineIndexEndpoint.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform\n",
        "\n",
        "        logger.debug(f\"Creating endpoint with id {endpoint_id}.\")\n",
        "        return aiplatform.MatchingEngineIndexEndpoint(\n",
        "            index_endpoint_name=endpoint_id,\n",
        "            project=project_id,\n",
        "            location=region,\n",
        "            credentials=credentials,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_gcs_client(\n",
        "        cls, credentials: \"Credentials\", project_id: str\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a GCS client.\n",
        "\n",
        "        Returns:\n",
        "            A configured GCS client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import storage\n",
        "\n",
        "        return storage.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_bigtable_client(\n",
        "        cls, credentials: \"Credentials\", project_id: str\n",
        "    ) -> \"bigtable.Client\":\n",
        "        \"\"\"Lazily creates a BigTable client.\n",
        "\n",
        "        Returns:\n",
        "            A configured BigTable client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import bigtable\n",
        "\n",
        "        return bigtable.Client(credentials=credentials, project=project_id,  admin=True)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_index_client(\n",
        "        cls, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a Matching Engine Index client.\n",
        "\n",
        "        Returns:\n",
        "            A configured Matching Engine Index client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        PARENT = f\"projects/{project_id}/locations/{region}\"\n",
        "        ENDPOINT = f\"{region}-aiplatform.googleapis.com\"\n",
        "        return aiplatform_v1.IndexServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT),\n",
        "            credentials=credentials\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_index_endpoint_client(\n",
        "        cls, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a Matching Engine Index Endpoint client.\n",
        "\n",
        "        Returns:\n",
        "            A configured Matching Engine Index Endpoint client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        PARENT = f\"projects/{project_id}/locations/{region}\"\n",
        "        ENDPOINT = f\"{region}-aiplatform.googleapis.com\"\n",
        "        return aiplatform_v1.IndexEndpointServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT),\n",
        "            credentials=credentials\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _init_aiplatform(\n",
        "        cls,\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        gcs_bucket_name: str,\n",
        "        credentials: \"Credentials\",\n",
        "    ) -> None:\n",
        "        \"\"\"Configures the aiplatform library.\n",
        "\n",
        "        Args:\n",
        "            project_id: The GCP project id.\n",
        "            region: The default location making the API calls. It must have\n",
        "            the same location as the GCS bucket and must be regional.\n",
        "            gcs_bucket_name: GCS staging location.\n",
        "            credentials: The GCS Credentials object.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform\n",
        "\n",
        "        logger.debug(\n",
        "            f\"Initializing AI Platform for project {project_id} on \"\n",
        "            f\"{region} and for {gcs_bucket_name}.\"\n",
        "        )\n",
        "        aiplatform.init(\n",
        "            project=project_id,\n",
        "            location=region,\n",
        "            staging_bucket=gcs_bucket_name,\n",
        "            credentials=credentials,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_default_embeddings(cls) -> TensorflowHubEmbeddings:\n",
        "        \"\"\"This function returns the default embedding.\n",
        "\n",
        "        Returns:\n",
        "            Default TensorflowHubEmbeddings to use.\n",
        "        \"\"\"\n",
        "        return TensorflowHubEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVjqayTbHI6D"
      },
      "source": [
        "# Create Bigtable\n",
        "\n",
        "https://cloud.google.com/bigtable/docs/creating-instance#console\n",
        "\n",
        "Use UI or gcloud, I just use UI to quickly create a bigtable instance\n",
        "\n",
        "create table:\n",
        "- name: vectordb\n",
        "- column family id: vector_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SwRzK3bW2vC"
      },
      "outputs": [],
      "source": [
        "! gcloud bigtable instances create INSTANCE_ID \\\n",
        "    --display-name=DISPLAY_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGEzlDzVkma7"
      },
      "outputs": [],
      "source": [
        "! gcloud bigtable instances tables create vectordb \\\n",
        "    --instance=INSTANCE_ID \\\n",
        "    --project=PROJECT_ID \\\n",
        "    --column-families=\"vector_text\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t5yDUm7K1nG"
      },
      "source": [
        "# Add texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_HxgBotR_nQ"
      },
      "outputs": [],
      "source": [
        "# @title in case you need the project information\n",
        "#PROJECT_ID = \"argolis-lsj-test\" # @param {type: \"string\"}\n",
        "#LOCATION = \"us-central1\"\n",
        "#ME_REGION          = \"us-central1\"\n",
        "#ME_INDEX_NAME      = \"langchain-vs-tencent\" # @param {type: \"string\"}\n",
        "#ME_DIMENSIONS      = 768 # when using Vertex PaLM Embedding\n",
        "#ME_EMBEDDING_DIR   = \"gs://langchain-vs-tencent-bucket\" # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "keCFtXCuK7om"
      },
      "outputs": [],
      "source": [
        "# @title get matching engine index information\n",
        "import matching_engine_utils\n",
        "mengine = matching_engine_utils.MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)\n",
        "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxdx8_hrHKoa"
      },
      "outputs": [],
      "source": [
        "# @title bigtable parameters\n",
        "BIGTABLE_INSTANCE_ID: str = \"me0920\" # @param {type: \"string\"}\n",
        "#request_per_minute = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "idlENxiGCqZO"
      },
      "outputs": [],
      "source": [
        "# @title import libraries\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "import matching_engine_bigtable\n",
        "from matching_engine_bigtable import MatchingEngine\n",
        "import vertexai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdiPbTYCLrdc"
      },
      "outputs": [],
      "source": [
        "# @title init vertexai and embedding function\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "embedding = VertexAIEmbeddings()\n",
        "# embedding.model_name = \"textembedding-gecko@latest\" # for enhanced quality of embedding\n",
        "# embedding.model_name = \"textembedding-gecko-multilingual@latest\" # for multilingual embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WoKa72_Gm76"
      },
      "outputs": [],
      "source": [
        "# @title get vector search instance\n",
        "me_instance = MatchingEngine.from_components(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=LOCATION,\n",
        "    gcs_bucket_name=ME_EMBEDDING_DIR,\n",
        "    index_id=ME_INDEX_ID,\n",
        "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
        "    instance_id=BIGTABLE_INSTANCE_ID,\n",
        "    embedding=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE9FUSsGMcc3",
        "outputId": "8932c61f-f95b-47ee-a419-a715dc89765b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6670, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17958, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16967, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18964, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16974, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16911, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16166, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19172, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14686, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17969, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18082, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15567, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16689, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18450, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16234, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14101, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18727, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17387, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16927, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15729, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17549, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 17430, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18267, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14478, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16622, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 18440, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 14681, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16502, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 16483, which is longer than the specified 800\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 15412, which is longer than the specified 800\n"
          ]
        }
      ],
      "source": [
        "# @title split file\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "def build_me_file(file_path=None, chunck_size=1000, seperator=\"\\n\\n\"):\n",
        "    # This is a long document we can split up.\n",
        "    loader = TextLoader(file_path)\n",
        "    state_of_the_union = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator = seperator,\n",
        "        chunk_size = chunck_size,\n",
        "        chunk_overlap  = 0,\n",
        "        length_function = len,\n",
        "    )\n",
        "    texts = text_splitter.split_documents(state_of_the_union)\n",
        "    return texts\n",
        "\n",
        "texts = []\n",
        "for chapter_number in range(1,9):\n",
        "    texts += build_me_file(file_path=f\"/content/story.txt\", chunck_size=800)\n",
        "result = [doc.page_content for doc in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o1Fz9yrMlhK",
        "outputId": "d865216c-f412-49d9-8539-dd264e80794c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5',\n",
              " 'c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5',\n",
              " 'c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5',\n",
              " 'c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5',\n",
              " 'c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5',\n",
              " 'c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5',\n",
              " 'c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5',\n",
              " 'c994557e05938b6882ffa8805bcfed642c8a2e555e47600229b51c497e6cc409',\n",
              " 'd9459dc07d5c4ecf349a59ebbe881c84a424758ddbeb9b1f97a2e011da7a8970',\n",
              " '2c3ae81472ca401a685a0de41d89af7a4e2e7e1301d3e63aefb6dd0b9c885ace',\n",
              " '91a725c48659e7175f4f9b311e248df978a783a4e5ab551a48d00affcda40290',\n",
              " 'f66dbea6708b869d7cabf29959a040f702c5604870cf6003d642af07d595e2f3',\n",
              " '3ed84033f23f72ebe855a53fa67b5741700ddaa1d4e25827ba7b03e043384b23',\n",
              " '9c36b362f47826254a597deeb2f363883e3f8d1b343f816f59c42d0de31e2cd8',\n",
              " '6cf6806265757e9ac790feb41d0eacffdd95dfaff1d300a7a3ee7847ad9730bd',\n",
              " 'eec241697c8e5e94b43c12bd348a3e476e032017ee5444d1da9d6d144f2aa378',\n",
              " '14663cce15b212a95ca2196de26b7fc33c4089aa443ba3c5ccd044f7ac7dbd01',\n",
              " '4674fc6bcc30a29602c0a4cb1fa7daa7e7a42fe4ed94bb131d0cbaaaf9088423',\n",
              " '321414203d6e3f63059e018b41f0eb785a89f17c68e2498a7f305514623a9fce',\n",
              " '05d2ebfcf0a2dea28ae803aad8b630d6895e442eb656f80a07f5620638f1bd85',\n",
              " '0954b3a087b47b84324238e56931ed1bfa5f50c309caf6352091ae1d93aa791b',\n",
              " '4b8b71b22f34587a39626987be6212d01565f9cd19abb0e52e427d81483f7710',\n",
              " '4b2fa0df483848d33201c9985324c8355e7e70b3471c92c881c4b7fc6d729842',\n",
              " '8e16a2e7a841b82aa76e7ffe34074923bc13013c4377e39551ece1834ec2a694',\n",
              " '00a9d74ff45bc64e0b76d3b307010637a6cfd3370a4659d78814f8d891956570',\n",
              " 'bf8c79c57715215dc9c293831985356b11178c3899b9c33aaa7d1c43b0298bd2',\n",
              " '0fda4611f4aa0f35454dda7a590393527bdd220d9907d3957c25cc16b3379d33',\n",
              " '2fcf4b8f9a3dfae5dae2837238400dc97930712ad4fab143cec9883d52dd0f23',\n",
              " '36abdc0d695d318bfd944ca170c4af0a43ee0ee1cef39d59cd56c47e362d7a98',\n",
              " 'e613c13ef052024bfb1a4d2aefddb8bd3623f5ced21b6d47f158bcf996100f54',\n",
              " '14d55cf73ce94088cc64d9b8216de197cf213a301dd02d63858b4729fbcb3c88',\n",
              " '022f295ca930eb2a0055616651c490ac2c06fe7197d84d475d98b9af1046ed79',\n",
              " 'f9226d1f830608c31599ababb28f597cdf134fc0506e3795ec42799904289129',\n",
              " 'bcca325944c585811d6ece4e5464b58fc00bc2ca45d3b5f5794b4e975a5e75dc',\n",
              " 'c3d76a291871b0f080c01da061052b1a064bb461ef42b02e06bac1028a803cba',\n",
              " '712ee139f5638c86761fcf0f2af7dac009b9f1e49ce9356a747370e6619602ba',\n",
              " 'b9adc297bcfd839f15866781d2a288371a89050b917c6d83a6094cffbb767794',\n",
              " 'fac3b43cf7c0b9996c316485e0ac7bf81b0bb148c9f69be76a13fa6f21297ce5']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title add text arrays, collection is supported in Vector Search\n",
        "me_instance.add_texts(texts=result, collection=\"wow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTwzhDIbQPu9"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke2GJHASQLnB",
        "outputId": "3331ba48-46d3-48de-ead1-3d867b0cb51c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='sorry. Please, join me. We still have things to settle before I leave,\\ndon\\'t we?\"\\n\"First thing is food,\" Greymane said, and he pulled up a chair and\\nspeared some chicken.\\n\"You and Wyll are colluding against me,” Anduin sighed. “The sad\\nthing is, I\\'m glad of it.\"\\nGenn grunted in amusement as Anduin filled his own plate. \"I\\'ve got\\nthe papers drawn up,\" Genn said.\\n\"Thank you for handling that. I\\'ll sign them right away.\"\\n\"Read them first. Doesn\\'t matter who wrote it. There\\'s a free piece of\\nadvice for you.”\\nAnduin smiled tiredly. \"You\\'ve given me quite a bit of free advice.\"\\n\"And some you\\'re even grateful for, I imagine,” Genn said.\\n\"All of it. Even what I disagree with and choose to ignore.\"\\n\"Ah, now there speaks a wise king.\" Greymane reached for the bottle\\nof wine on the table and filled his glass.\\n\"No coup planned, then?” Anduin found himself reaching for\\nanother helping of chicken. His body was hungry, it would seem, even\\nif his mind was distracted.\\n\"Not this visit.\"\\n\"That\\'s good. Save your plotting efforts for another time.\"\\n\"There is one thing I\\'d like to discuss before you depart,\" Greymane\\nsaid, turning serious. There was something in his body language that\\nalerted Anduin, who put down his knife and fork and regarded the\\nother king.\\n\"Of course,\" Anduin said, concerned.\\nNow that he had the full attention of the king of Stormwind, Genn\\nlooked a bit uncomfortable. He took a drink of the wine, then faced\\nAnduin squarely.\\n\"You honor me with your trust,” he said. “And I\\'ll do everything I\\ncan to govern your people with care and diligence if, Light forbid,\\nsomething should happen to you.\"\\n\"I know you will,\" Anduin assured him.\\n\"But I\\'m an old man. I won\\'t be around forever.\"\\nAnduin sighed. He knew where this was heading. \"It\\'s been a long\\nand challenging day. I\\'m too tired to have this discussion with you.\"\\n\"You\\'ve always been too something or other every time I\\'ve brought\\nit up,\" Genn pointed out. Anduin knew it was true. He toyed with his\\nfood. \"We\\'re on the eve of your departure to visit several different\\nlands,\" Greymane continued. “Fresh dangers are cropping up. When\\nwill be a good time? Because I don\\'t relish the thought of trying to sort\\nthrough gaggles of nobility each pushing their best claim forward.\"\\nThe image made Anduin smile in spite of himself, but it faded at\\nGenn\\'s next words.\\n\"This isn\\'t a game. If the wrong person is given the kingdom,\\nStormwind could find itself looking at a very dire situation indeed.\\nYour mother was a horrible casualty of an angry mob furious at what\\nthe nobility was doing to the people. And you are old enough to\\nremember how unstable things were when your father went missing.”\\nAnduin was. He\\'d been the nominal king during his father\\'s\\ndisappearance, but he\\'d had Bolvar Fordragon standing by his side to\\noffer advice. Varian had gone missing, and the black dragon Onyxia\\nhad replaced him with an impostor, ruling the kingdom through that\\npuppet. Stormwind was unsettled and tumultuous until Onyxia was\\ndefeated and the real Varian Wrynn again sat upon the throne.\\nThe young king took a sip of his wine. \"I remember, Genn,\" he said\\nquietly.\\nGenn gazed down at his half-eaten meal. \"When I lost my boy,\" he\\nsaid softly, his voice intense, \"I lost a piece of my soul. I didn\\'t just love\\nLiam. I admired him. I respected him. He would have been a\\ntremendous king.”\\nAnduin listened.\\n\"And when he fell-when that heartless, undead banshee killed him\\nwith an arrow meant for me-so much died with him. I thought I\\nwould never recover. And I didn\\'t...not completely. But I had my wife,\\nMia. I had my daughter, Tess, every bit as strong and smart as her\\nbrother.\"\\nAnduin did not interrupt. Genn had never been so open with him\\nbefore. Now the Gilnean king lifted his blue eyes. In the candlelight,\\nthey shimmered, and his voice was husky with emotion.\\n\"I moved on. But I had a hole in my heart where he used to be. A\\nhole I tried to fill with my hatred for Sylvanas Windrunner.\"\\nGently, Anduin said, “That kind of hole can\\'t be filled with hate.\"\\n\"No. It can\\'t. But I met another young man who loved his people as\\nLiam did. Who believed in things that were good, and just, and true. I\\nfound you, my boy. You\\'re not my Liam. You\\'re yourself. But I do\\ncatch myself trying to guide you.\"\\n\"You can\\'t replace my father, and I know you know that,” Anduin\\nsaid, deeply moved by Genn\\'s words. “But you\\'re a king and a father\\nboth. You understand being both. And it helps.\"\\nGenn cleared his throat. Emotions were no stranger to him, Anduin\\nknew, but they were usually the hot, angry, violent ones. It was part of\\nthe worgen curse, yes, but Anduin knew it was also an intrinsic part of\\nthe man. Genn was not used to the softer emotions and almost always,\\nas he did now, chased them away.\\n\"I\\'d be saying the same thing to Liam right now if he were here. Life\\nis too short. Too unpredictable. For anyone in this world, especially for\\na king. If you love Stormwind, you need to make sure it\\'ll go into\\nhands that will care for it.”\\nHe paused. Here it comes, Anduin thought.\\n\"Anduin, is there anyone you\\'ve considered as a possible queen?\\nSomeone to rule in your stead should you fall in battle, bear a child to\\ncarry on the Wrynn bloodline?\"\\nAnduin abruptly grew keenly interested in the food before him.\\nGenn sighed, but it came out as more of a growl. “Times of peace are\\nrare in this world. And they\\'re always too brief. You need to use this\\ntime to at least start the search. If you\\'re traveling to all these places,\\ncouldn\\'t you have a few formal dances, or theater visits, or\\nsomething?\"\\n\"Believe it or not, I understand I need to do that,\" Anduin admitted.\\nGenn did not know about the small box with Queen Tiffin\\'s rings that\\nAnduin kept close, and the younger man wasn\\'t about to volunteer\\nthat information. \"And the answer is no, I\\'ve not met anyone yet that\\nI\\'ve felt that way about. There\\'s time. I\\'m only eighteen.”\\n\"It\\'s not uncommon for royal betrothals to occur when the\\nparticipants are still in the cradle,\" Genn pressed. \"I\\'m a bit of a\\nstranger to Stormwind society, but surely there are others who could\\ncompile a list.\"\\nGenn meant well, Anduin knew. But he was weary and worried, and\\nhis focus was on what to do with a wounded world, not on an arranged\\nmarriage.\\n\"Genn, I appreciate your concern,” he said, choosing his words with\\ncare. “This is not an unimportant matter. I\\'ve told you I understand\\nthat. But the idea of an arranged marriage-agreeing to spend my life\\nwith someone I may not even know before making that commitment—\\nit\\'s abhorrent to me. Besides,\" he added, \"you didn\\'t have one.\\n99\\nGenn scowled. \"Just because it\\'s not a path I chose doesn\\'t mean it\\'s\\nnot a sound one. I know it\\'s not the most romantic thing in the world,\\nbut it doesn\\'t have to be some stranger. My daughter, Tess, is close to\\nyour age. She would make-\"\\n\"Quite the protest were she here at this moment,\" Anduin\\ninterrupted. \"From the little I\\'ve seen of her, it\\'s clear she\\'s a\\nremarkable woman. But she certainly has her own life, and I\\'m going\\nto take a wild guess and say that I don\\'t think queen of Stormwind is\\nhigh on her list of what she wants for it.\"\\nTess Greymane, a few years his senior, was by all accounts a strong-\\nwilled woman. There had been all kinds of rumors about her actions,\\nimplying that she had taken a page or two from Mathias Shaw. He had\\nnot asked Genn about it, and now that the man had put forth his\\ndaughter as a potential queen, he wasn\\'t about to.\\nGenn\\'s white brows drew together in a frown. “Anduin-\\'\\n\"\"\\n\"We will revisit this topic, I promise. But for now, there\\'s another\\nargument I\\'d like to get into with you.\"\\nDespite himself, Genn chuckled. “You know I\\'ll argue with you any\\ntime, Your Majesty.\"\\n\"I do indeed,\" Anduin said, “and especially about this. After Magni\\'s\\nvisit, Moira, Velen, and I went to the Netherlight Temple. I don\\'t think\\nit would surprise you one whit to tell you that I found it to be...\" He\\nshook his head. \"Truthfully, words fail me. It was serene and beautiful,\\nand simply being there made me feel so peaceful. So focused.\"\\n\"The only surprise I have about your visit was how long it took you\\nto get there,\" Genn said. “But then again, a king has little time for\\nserenity and peace.\"\\n\"While I was there, I met two people who surprised me,\" he said. He\\ntook a breath. Here we go, he thought. \"One of them was Calia\\nMenethil.\"\\nGenn stared. \"Are you certain? Not an impostor?\"\\n\"She looks a great deal like her brother. And I trust the priests of the\\ntemple have made sure her claim is true.\"\\n\"You place a lot of faith in the priests\\' goodwill.\"\\nAnduin smiled. \"Yes, I do.\"\\n\"Well, out with it. What did you learn? How did she escape? Does\\nshe still lay claim to the throne of Lordaeron, provided we can one day\\nevict those rotting squatters who currently defile it?”\\nAnduin smiled a bit ruefully. “I didn\\'t press. I\\'ll return and speak\\nwith her later. I got the impression that it wasn\\'t a happy story.\"\\n\"Light knows it couldn\\'t be,\" Genn said. \"That poor family. What the\\ngirl must have been through. Probably escaped those wretches by the\\nskin of her teeth. How she must despise the undead after that!”\\n\"Actually, that\\'s the next thing I wanted to tell you.The Netherlight\\nTemple is a hall for Azeroth\\'s priests. All of its priests. Including\\nHorde.” He paused. “Including Forsaken.”\\nAnduin had braced himself for an angry bellow of protest. Instead,\\nGenn calmly put down his fork and spoke in a carefully controlled\\nvoice.\\n\"Anduin,\" he said, “I understand that you always want to see the\\nbest in people.\"\\n\"It\\'s not-\"\\nGenn held up a hand. \"Please, Your Majesty. Hear me out.\"\\nAnduin frowned but nodded.\\n“It\\'s an admirable trait. Especially in a ruler. But a ruler must be\\ncareful that he\\'s not played for a fool. I know you met and respected\\nThrall. And I know you consider Baine a friend, and he has acted with\\nhonor. Even your father negotiated with Lor\\'themar Theron and held\\nVol\\'jin in high esteem. But the Forsaken are...different. They don\\'t feel\\nthings like we do anymore. They\\'re...abominations.\"\\nAnduin\\'s voice was mild. \"A current leader of the Conclave is\\nArchbishop Faol.\"\\nGenn swore and sprang to his feet. Silverware clattered to the floor.\\n\"Impossible!\" His face had flushed, and a vein stood out on his neck.\\n\"That\\'s worse than an abomination. That\\'s blasphemy! How can you\\ntolerate this, Anduin? Doesn\\'t it sicken you?\"\\nAnduin thought about the impish good humor the late Alonsus Faol\\nhad displayed. The kindness, the concern. We are priests before all\\nelse. And he was.\\n\"No,\" Anduin said, smiling. “Quite the opposite. Seeing them there,\\nin that place of Light...it gave me hope, Genn. The Forsaken aren\\'t\\nmindless Scourge. They\\'re people. They have free will. And yes, some\\nof them have been changed for the worse. Those have moved on in\\ntheir new existence with hate and fear. But not all of them. I saw\\nForsaken priests speaking not only with tauren and trolls but with\\ndwarves and draenei. They remembered the good. Moira\\'s worked\\nwith Faol for some time now, and-\"\\nGenn swore. \"Moira, too? I thought dwarves had sense! I\\'ve heard\\nenough.\" He turned, about to stalk out of the room.\\n\"No, you haven\\'t.\" Anduin\\'s voice was soft but brooked no\\ndisagreement. He held out a hand and indicated the chair the other\\nhad vacated. \"You\\'ll stay, and you\\'ll listen.\"\\nGenn eyed him, surprised, then nodded in approval as he sat back\\ndown, albeit with obvious reluctance. He took a deep breath.\\n\"I will,\" he said. “Though I won\\'t like it.\"\\nAnduin leaned forward intently. \"There\\'s an opportunity here if\\nwe\\'re bold enough to take it. Sylvanas gave the Forsaken life. Of course\\nthey follow her. But the Alliance turned away from them. All we had to\\noffer them were names-deaders,\\' \\'rotters.\\' We viewed them with fear.\\nDisgust. We couldn\\'t even fathom that they were people.\\n\"Were,\" Genn said. \"They were people. Once. They\\'re not any\\nlonger.\"\\n\"We\\'ve chosen to see them that way.\"\\nGenn tried another tactic. \"All right.\" He leaned back in his chair,\\neyes narrowed. “Let\\'s say that you saw a few decent Forsaken, an\\nextremely small handful, all of whom happened to be priests. Have\\nyou encountered any others like that?\"\\nThere was another Anduin recalled who had most definitely not\\nbeen a priest. At the trial of Garrosh Hellscream, the bronze dragons\\nhad offered both the defense and the prosecution the ability to show\\nscenes of the past through an artifact known as the Vision of Time. In\\none such vision, Anduin had witnessed a conversation between a\\nForsaken and a blood elf in a tavern shortly before that tavern had\\nbeen destroyed by those too devoted to Hellscream.\\nThe two soldiers had been against the violence and cruelty that\\nGarrosh had personified. And they had died for their beliefs. Oh, what\\nwas the name...It began with an \"F.\" \"Farley,\" Anduin said. \"Frandis\\nFarley.\"\\n\"Who?\"\\n\"A Forsaken captain who turned against Garrosh. He was outraged\\nby the violence of Theramore. He lived right here, in Stormwind, when\\nhe was alive.”\\nGenn looked as though he couldn\\'t even comprehend what Anduin\\nhad just said.\\n\"Frandis Farley wasn\\'t a priest. Just a soldier who still had enough\\nhumanity left in him to understand evil when he saw it.\" The more\\nAnduin worked it out, the more certain he became.\\n\"Anomalies,\" Greymane said.\\n\"I don\\'t accept that,” Anduin said, leaning forward. “We have no\\nidea what the average citizen of the Undercity thinks or feels. And one\\nthing you cannot argue with me: Sylvanas cares about her people.\\nThey matter to her. And that may be something we can use to our\\nadvantage.\"\\n\"To bring her down?\"\\n\"To bring her to the negotiating table.” The two men regarded each\\nother, Anduin calm and focused, Genn struggling to suppress his\\nanger.\\n\"Her goal is to turn more of us into more of them,\" Genn said.\\n\"Her goal is to protect her people,” Anduin insisted. “If we let her\\nknow we understand that motivation, if we can assure her that those\\nwho already exist would never be in danger from the Alliance, she\\'s\\ngoing to be a lot less likely to use Azerite to create weapons to kill us.\\nEven better, we might actually be able to work with the Horde to save\\na world we both have to live in.”\\nGenn looked at him for a long moment. \"You sure you didn\\'t catch\\nsomething in Ironforge?”\\nAnduin held up a placating hand. \"I know it sounds like madness.\\nBut we\\'ve never tried to understand the Forsaken. Now could be the\\nperfect chance. Archbishop Faol and the others could help open\\nnegotiations. Each side has something the other just might want.”\\n\"What do we have that the Forsaken want? And what do the\\nForsaken have that we could possibly want?\"\\nAnduin smiled, gently. His heart was full as he answered, “Family.”\\nHis quarters were dark as he entered them, illuminated only by the\\nlight of the moons. \"You got my message,” Anduin said aloud as he lit\\na single candle and looked around.\\nThe room appeared empty, but of course it wasn\\'t. A shadow that\\nhad seemed perfectly ordinary a moment earlier shimmered, and a\\nfamiliar lithe frame stepped into the faint light.\\n\"I always do,\" said Valeera Sanguinar.\\n\"One of these days I\\'m going to ask you to show me how you get in.\"\\n\"\"\\nShe smiled. \"I think you might be a little too heavy to manage it.\\nAnduin chuckled. He counted himself fortunate that there were\\nmany people he trusted. Not all kings, he knew, could say the same\\nthing. But Valeera was on an entirely different level from even Velen or\\nGenn Greymane. She and Varian had fought alongside each other in\\nthe gladiator pits, and Anduin had met her years ago. She had saved\\nboth his and his father\\'s lives on more than one occasion and had\\npledged her loyalty to the Wrynn line. And what was almost as\\nimportant was that she was able to move in circles denied to Anduin\\nand his advisers.\\nValeera was a blood elf, and she was the king\\'s personal spy.\\nShe had served Varian in that capacity during his reign, and she had\\naided the prince when he needed messages delivered that he asked be\\nkept secret even from his father. Although he trusted Spymaster Shaw\\nto do what was best for the kingdom, Anduin didn\\'t know the man well\\nenough to trust that he would do what was best for the king. Certainly\\nhe would not have approved of the correspondence Anduin had been\\ncarrying on for the last few years.\\n\"I assume you know about Azerite,\" he said.\\nValeera nodded her golden head, perching on a chair without\\nwaiting to be asked. \"I do,\" she said. \"I hear it can build kingdoms,\\nbring them down, and possibly doom the world.\"\\n\"All that is true,\" Anduin confirmed. He poured them each a cup of\\nwine and handed one to her. “I\\'ve never embraced the idea that Horde\\nand Alliance must always be against each other. And it seems to me\\nthat now, more than ever, we have to have cooperation and trust on\\nboth sides. This new material...\" He shook his head. \"Far too\\ndangerous in the hands of any enemy. And the best way to defeat an\\nenemy is to make them a friend.\"\\nThe blood elf sipped her wine. “I serve you, King Anduin. I believe in\\nyou. And I am most certainly your friend and always will be. I would\\nlike to live in this world that you see. But I don\\'t think it\\'s possible.\"\\n\"Improbable,\" Anduin said, “but I do think it\\'s possible. And you\\nknow better than anyone that I\\'m not alone in that sentiment.”\\nHe handed her a letter. It was written in a personal code understood\\nby only a handful of individuals. Valeera took it and read. Her\\nexpression soured, but she nodded as she carefully tucked it into a\\npocket close to her heart. As always, she would memorize the contents\\nin case the letter was lost or destroyed.\\n\"I will see that his surrogate receives it,” Valeera promised. She did\\nnot look happy.\\n\"Be careful,\" she added. \"No one will support this. It\\'s doomed to\\nfailure.\"\\n\"But what if it works?\" Anduin pressed.\\nValeera peered into the ruby depths of her cup, then lifted her\\nglowing eyes to his. “Then,” she said slowly and with deep reluctance,\\n“I think I might have to stop using the word \\'impossible.\\'\\n9 99', metadata={}),\n",
              " Document(page_content='fueled by Azerite won\\'t wipe out every one of us.\"\\nThe days passed. Anduin and Calia continued to meet with those\\nwhose names were on the provided list. Some were like Fredrik:\\nindividuals who struggled with the concept of a Forsaken as a \"person\"\\nbut yearned for connection. Others, though they might have expressed\\na willingness to meet with their Forsaken kin in the letter, were\\ndeemed unsuitable. Calia was a keen observer, and Anduin trusted the\\nold injuries he had received from the Divine Bell to guide his\\ndecisions. And sometimes, sadly, it was quite obvious that the\\n\"reunion\" would have resulted in violence.\\nThere was an undercurrent of hostility, an unvoiced desire to punish\\nthe Forsaken simply for the act of having died and been reborn.\\nOthers, usually with more than sufficient reason, were openly angry at\\nSylvanas. They were given coin and refreshment for their time and\\ndismissed.\\n“Hate,” Anduin said once to Calia, \"always surprises me. It\\nshouldn\\'t. But it does.\"\\nShe nodded her golden head sadly. “As priests, we cannot harden\\nour hearts and still do what the Light would have us do. Vulnerability\\nis our strength and our weakness both. But I would have it no other\\nway.\"\\nThe candles had burned low in the chamber on the final day as the\\nlast person settled into the chair. Her name was Philia Fintallas, and\\nthe person who had asked for her was her father, Parqual.\\nPhilia looked to be about fifteen years old, if that. She had large,\\nexpressive eyes and a small button nose. With the vibrancy of her\\ndemeanor, she seemed as far removed from a Forsaken as the summer\\nfrom the winter.\\n\"My father was a historian in Lordaeron, and I was born there,\" she\\nsaid. \"But we had family here-aunts, uncles, cousins-and I had come\\nback for a visit. I was supposed to have gone home the day after—” She\\nbroke off, and tears welled in her eyes. Anduin fished out a\\nhandkerchief and handed it to her. She accepted it with a trembling\\nsmile of thanks and sipped at the water Calia had poured for her.\\n\"After Arthas came,\" Anduin finished for her. He sneaked a glance at\\nCalia. He couldn\\'t count the number of times her brother\\'s name had\\nbeen mentioned during these meetings with survivors. And every one\\nhad cursed him heartily. On some level, it had to wound that man\\'s\\nsister. Anduin never identified Calia by name, and she never reacted to\\nthe vile things that were said about the slain Lich King. He admired\\nher strength, particularly given what she had said about not hardening\\nher heart.\\nPhilia nodded miserably, then took a deep breath and continued.\\n\"We never heard anything from Mama or Papa, so we assumed they\\nwere dead. Hoped they were dead, given all we had heard about the\\nScourge. Oh, isn\\'t that horrible now that I know I have to tell you that\\nmy uncle didn\\'t want me to come when I got your letter, Your Majesty.\\nBut I had to. If by some miracle it\\'s still him, I have to see him. I have\\nto see my papa!\"\\nHer voice caught as the tears she had tried so hard to contain spilled\\ndown her cheeks.\\nCalia had unfailingly been kind and comforting to all she and\\nAnduin had spoken with, but this girl\\'s obvious love clearly struck her\\npowerfully. She rose and went to Philia, holding her tightly, letting her\\nsob against her shoulder. Anduin thought he glimpsed tears in the\\npriestess\\'s eyes as the two women clung to each other, and a thought\\nstruck him. It was a delicate subject but one he needed to broach with\\nCalia once their task here was completed.\\n\"It\\'s true, I promise you,\" he said to Philia. “I haven\\'t met your\\nfather, but I have encountered many Forsaken who remember who\\nthey were and who would be very happy to be reunited with those who\\nhave thought them dead or destroyed beyond recognition.\"\\nCalia stood back a step from the girl, placing her hands on her\\nshoulders. \"Philia? Look at me.\"\\nThe girl did so, gulping, her eyes red and swollen. \"I have heard of\\nyour father from someone who knows him as he is now. He speaks\\nvery highly of him and tells me he is still kind and intelligent. I believe\\nit will be a joyful reunion for you both.\"\\n\"Thank you! Thank you so much! When will this happen?\"\\n\"We will send a courier with instructions,\" Anduin promised her.\\n\"Hopefully, not too long.\"\\nWhen the girl left, beaming with joy, Calia gave Anduin a smile even\\nthough her face was still flushed from the empathetic tears she had\\nshed.\\n\"I hope you see now what good you do, Anduin Wrynn.\"\\nHe gave her a lopsided grin. “I hope it will be good,\" he said. “I\\'ll\\nrelax when it\\'s all over. I couldn\\'t have done this without you, Calia.\\nYou have a gift for reading people.\"\\n\"That was something I learned from an early age as a royal child, as\\nI\\'m sure you did. Working so closely with so many fellow priests has\\nonly helped to hone that skill and temper it with compassion.\"\\nThere was a pause. Calia herself had just provided him a segue into\\nthe conversation he wished to have with her, but even so, Anduin\\nsteeled himself.\\n\"Calia,\" he began carefully, \"you have been a tremendous help. And\\nyou aren\\'t a Stormwind citizen. If this plan does lead eventually to\\npeace, you\\'ll be a hero of the Alliance.\"\\nShe smiled a touch ruefully. “Thank you, but I don\\'t consider myself\\na member of the Alliance. I\\'m a citizen of nowhere now except perhaps\\nthe Netherlight Temple,” she said. “I go where the Light wills me. I\\ntruly believe this is the right path toward mending other, greater rifts.\"\\nAnduin couldn\\'t let it go without making absolutely certain. Too\\nmuch was at stake. \"The kingdom of Lordaeron is your birthright. Few\\nwould be willing to let go of such a title and the power it would grant\\nthem,\" he pressed. \"I understand your reasoning, but many do not.\\nYou may have some nationalist champions rising, ready to take the\\ncity in your name.\"\\nSuddenly her expression grew thoughtful, and she searched his eyes.\\n\"Would you be among them, Anduin? Is that why you ask? Would the\\nking of Stormwind make war on the Horde, scour the Undercity, to\\ngrant the queen of Lordaeron her empty kingdom?\"\\nThe throne was hers by every right. Yet was it worth war should she\\nexpress a desire to claim it? She saw the struggle on his face and put a\\nhand on his.\\n\"I understand. Don\\'t worry. Those who currently inhabit Lordaeron\\nlived there in life. The Forsaken are the true heirs. It belongs to them\\nnow. The best I can do for those whom I would have ruled is exactly\\nwhat I\\'m doing. I\\'ve found peace and a calling where I can really\\nmatter. That\\'s more important than a bloodied crown.\"\\n\"Sacrificing peace and a calling is usually the price of a crown,\"\\nAnduin said.\\n\"You have not let it be so. Stormwind is fortunate to have you. But if\\nyou truly wish to thank me, I have a favor to ask. Of both you and the\\narchbishop. I\\'d like to participate in the Gathering.”\\nAnduin frowned slightly. “I don\\'t think that is wise,\" he said. \"There\\nmay be those who recognize you. It could be dangerous. It could be...\\nmisconstrued.\" It could, in fact, lead to war.\\n\"If any of the Forsaken do recognize me, it will give me the chance to\\nshow that I bear them no ill will,\" she countered. \"That I have no\\ndesire to run them out of the place that\\'s been their home for so long. I\\nwant them to stay there. I want them to be safe.\"\\nAnduin watched her carefully, taking a breath and centering\\nhimself. Light-let me know if she means them harm. He felt no\\nresponding ache in his bones, no hint that Calia Menethil was\\nplanning some kind of murderous coup. Her intentions were in\\nalignment with the Light they both served.\\n\"I\\'ve already established a bond of trust with these people we\\'ve\\ninterviewed,\" she continued. “And no one knows the archbishop better\\nthan I do.\"\\nThis was true. And no one knew her better than Faol. “I will speak\\nwith the archbishop,” Anduin said at last. “If he is agreeable, then I\\nam, too.\"\\nCalia beamed at him. \"Thank you,\" she said. \"It means more than\\nyou know.\"\\nThere was one last thing he felt compelled to say. “I have a question,\\nand it\\'s important that I know the answer.\"\\nHer golden hair, as golden as that of Arthas, as golden as his own,\\nfell in a bright sheet to hide her face as she looked down. Her voice was\\nsmall when she spoke.\\n\"I trust you, Anduin,” she said. \"If you feel you must know the\\nanswer, then ask it.”\\nHe took a deep breath.\\n\"Calia... Is there a child? Do you have an heir?\"\\nCHAPTER TWENTY-FIVE\\nStormwind\\nhe unspoken words hung between them, heavy and sad, and\\n\"There was a child,” Calia Menethil said so softly that he had to\\nstrain to hear her. It was enough, but Anduin waited to see if she was\\nready to tell her story. Just as he drew breath to change the subject,\\nshe began to speak.\\n\"You must understand...my father was ordinarily a kind and\\nunderstanding man, but on this one thing he was firm. He was to\\nchoose the man I was to marry, and I was to agree to it.\"\\nHer sorrowful sea-green eyes lifted from the clasped hands in her\\nlap. \"I have made many mistakes and poor choices in my life.\\nEveryone has, but as royalty, our decisions matter more than those of\\nothers because they affect so many more people. You may feel that you\\nhave to find a queen, have an heir. Your advisers will want you to make\\na good political match. Others might be able to live with such things.\\nBut not people like us. Promise me this, Anduin: whatever anyone tells\\nyou to do, don\\'t marry if your heart doesn\\'t tell you to.\"\\nHer face was fierce but still beautiful and haunted, and her words\\nstruck him with the power of truth. Even so, Anduin knew that in the\\nend he would have to do what was best for his kingdom.\\n\"I cannot make a promise I may not be able to keep,” he said, “but\\nfor what it\\'s worth, I share your feelings on this matter.”\\n\"We all do what we must,\" Calia said. \"I was not the direct heir. I\\ndon\\'t have your responsibilities. If I had, I might have agreed without\\nprotest. But Arthas was the heir, the firstborn son, and as he grew up,\\nPapa began to focus more on him. It seemed as though he and Jaina\\nwould be a perfect couple-a love match as well as a sound political\\none. At least until Arthas somehow decided that it wasn\\'t perfect.”\\nShe paused, then looked up at him. “Jaina...I\\'ve been afraid to ask\\nyou. Is she...”\\n\"She\\'s alive,\" Anduin hastened to reassure her. \"We don\\'t know\\nwhere she is, but she can take care of herself.\" He did not tell her of\\nJaina’s struggles or of her apparent abandonment of the Alliance.\\nCalia had enough sorrows on her heart. Anduin had no desire to add to\\nthem unless she inquired.\\nHis words seemed to be enough for her. She smiled, her eyes\\ndistant, and said, “I\\'m glad. She was dear to me when we were\\nyounger. When the world was less cruel than now. And with what\\nArthas...became...I am deeply glad she was not wed to him.\\n\"But while Father\\'s eye was on my brother, I conducted my own\\nquiet rebellion. I fell in love with someone Father never would have\\napproved of: one of the footmen. We stole what moments we could,\\nand once, in the dark of night, we slipped away and begged a priestess\\nto marry us. She refused at first, but we persisted. We came to her\\nagain and again, my sweet love and I, and at last, with the Light\\'s\\nblessing, we were wed.\"\\nHer hand fell to her belly, flat now but once rounded with child.\\n\"When I was certain that I was carrying, I confided in Mother. Oh, she\\nwas furious with me! But she could tell by my face that this was a true\\nlove, and I assured her my child would be legitimate. Father was too\\ncaught up in Arthas to make much objection when my mother and I\\nwent on a long rest\\' to more remote parts of the kingdom.\"\\nCalia\\'s hand ceased to move on her abdomen, and both hands curled\\ninto fists. \"I got to hold my beautiful little girl and tend to her for a few\\nweeks before it was decided that my husband would raise her, away\\nfrom Lordaeron and ignorant of her birthright. Mother promised that\\nwhen the time was right-when Arthas had finally married and\\nproduced an heir-we could acknowledge my daughter and perhaps\\nelevate my husband to a nobleman\\'s status so that her name would be\\nunsullied.\\n\"That day never came. But the Scourge did.\"\\nAnduin listened, his heart full of sympathy. Calia was describing\\nbeing sold off like livestock to the highest bidder. She\\'d rebelled, fallen\\nin love, and conceived a child. A daughter. For a brief moment, Anduin\\nwondered what a daughter or a son of his would look like. Regardless\\nof appearance or gender, that babe would rule one day...and until then\\nwould be deeply loved.\\n\"I don\\'t remember much of that time. I remember lying in a ditch\\nwhile the Scourge passed above me. I believe to this day it was thanks\\nto the Light that they never found me. I made my way to Southshore,\\nwhere my husband and child had been hidden away. We all three wept\\nwhen we were reunited. But it was not to last.\\n\"\"\\nNo. Not a second time. Anduin reached for one of her fisted hands.\\nFor a moment, it was tense beneath his, and then, slowly, the fist\\nunclenched as Calia allowed her fingers to entwine with his.\\n\"You don\\'t have to say anything else, Calia. I\\'m sorry I troubled\\nyou.\"\\n\"It\\'s all right,\" she said. “I\\'ve started now. I think I want to finish.”\\n\"Only if you wish,\" he assured her.\\nShe gave him a faint smile. “Maybe if I tell someone, the nightmares\\nwill stop.\"\\nInwardly, he winced; he had no response to that. She continued. \"No\\none recognized me. Everyone assumed I was dead. We were happy for\\na time. And then came the blight. We ran. I wasn\\'t about to leave my\\nfamily again, but in the crowd we were separated. I stood in the middle\\nof the street, screaming for them. Someone took pity on me, pulling\\nme onto his horse and galloping past the limits of the town barely in\\ntime.\\n\"There was a cluster of refugees in the forest. So many of us waited,\\ndesperate for word of our loved ones. Sometimes prayers were\\nanswered, and there were reunions that were...\" Calia bit her lip. \"I\\nprayed that my family, too, would be spared. But...\" Her voice trailed\\noff. \"I never saw them again.\"\\nAnd then, with a realization that stopped his breathing with shock,\\nAnduin understood why Calia had decided to befriend the Forsaken.\\nWhy, instead of seeing them as the destroyers of her city, her way of\\nlife, and all her family, she had chosen to identify with them.\\n\"You\\'re hoping that your husband and child, too, became Forsaken\\ninstead of dying as Scourge,” he said softly. \"You\\'re hoping you\\'ll get\\nword of them at the Gathering.”\\nCalia nodded, wiping at the tears on her face with one hand. The\\nother remained clasped with the young king\\'s. “Yes,” she said. “It\\nwasn\\'t until I met the archbishop that I started to understand that the\\nForsaken weren\\'t monsters. They were just...us. The same people you\\nand I would be if we had been killed and given a different sort of life.\"\\n\"You don\\'t know if your family would have been like that,\" Anduin\\ncautioned. \"They could have been driven mad or turned cruel. It might\\nbe devastating for you to see them.\" Genn\\'s words to Fredrik came\\nback to him now even as he spoke.\\n\"I know. But I have to hold out for the chance. Isn\\'t that what the\\nLight is all about, Anduin? Hope?\"\\nAnduin\\'s mind went back to the trial of Garrosh Hellscream. When\\nthat orc had executed his escape, he had done so thanks to the chaos\\nsown by an unexpected attack on the temple. In that battle, Jaina had\\nbeen severely wounded.\\nNo, he corrected himself. She had been dying.\\nSo many tried to heal her, both Alliance and Horde. But the wound\\nwas too much. Anduin remembered kneeling on the cold stone floor of\\nthe temple, watching Jaina\\'s labored breathing and seeing red bubbles\\nform on her lips, his hands on her bloodied robe. Please, please, he\\nhad prayed, and the Light had come. But he, like the others, was\\nexhausted. And the Light he had called would not be enough to save\\nher.\\nHe remembered others telling him to come away, that he\\'d done all\\nhe could. But he stayed there in those bleak, impotent moments before\\nthe death of this woman he\\'d loved as an aunt. No, he had told those\\nwho wanted him to walk away. I can\\'t.\\nAnd then the voice of his teacher-Chi-Ji, the Red Crane. And so, the\\nstudent remembers the lessons of my temple.\\nAnduin quoted Chi-Ji\\'s words to Calia now. \"Hope is what you have\\nwhen all other things have failed you,\" he said. \"Where there is hope,\\nyou make room for healing, for all things that are possible—and some\\nthat are not.\"\\nHer eyes shone, and she gave him a tremulous smile. \"You\\nunderstand,\" she said.\\n“I do,” he said. “And I know that having you participate in the\\nGathering is the right thing to do.\" As he spoke, he felt warmth and\\ncalmness steal through him. That warmth passed through their\\nclasped hands to Calia, and he saw the lines around her eyes and\\nmouth lessen, her body relax.\\nWhatever betided, this act of kindness was the right thing. Anduin\\nhad to hope that they would not pay too dear a price for it.\\nTANARIS\\nThe team of goblin engineer and gnome mineralogist picked up their\\npace. Saffy grilled Grizzek on everything he knew about his \"boss,\" and\\nit killed him to watch her face, normally so bright and cheerful—\\nespecially recently-grow darker and more withdrawn. Sometimes\\nGrizzek bridled at how his people were regarded or, more accurately,\\nreviled. Not all goblins were out to sell dangerous things at ludicrous\\nprices. There were some who were even well regarded: Gazlowe, who\\noperated out of Ratchet, south of Orgrimmar, came to mind.\\nBut Jastor Gallywix epitomized the worst that could possibly be said', metadata={})]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title similarity search demo\n",
        "me_instance.similarity_search(\"Who's Thrall's father?\", k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxwq__J6kDfp",
        "outputId": "3a9539b6-af9f-4e93-de6d-d87576c8eb59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title in case you want to delete the vector search collection\n",
        "#me_instance.delete_collection(collection=\"wow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LCy0q_X0Sjuf",
        "outputId": "ba64de9a-7afd-40d4-fd62-30bc700a7588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. The answer should be as detailed as possible. You have access to the following tools:\n",
            "\n",
            "Vector Search: search for a query\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Vector Search]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times. )\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question. The final answer should be very detailed.\n",
            "\n",
            "Begin!\n",
            "\n",
            "Previous conversation history:\n",
            "\n",
            "\n",
            "Question: Who's Thrall's father?\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Durotan is Thrall's father\n",
            "Action: Vector Search\n",
            "Action Input: father of thrall\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m The father of Thrall is Durotan. Durotan named the land Durotar for his father.\u001b[0m\n",
            "Thought:\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. The answer should be as detailed as possible. You have access to the following tools:\n",
            "\n",
            "Vector Search: search for a query\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Vector Search]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times. )\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question. The final answer should be very detailed.\n",
            "\n",
            "Begin!\n",
            "\n",
            "Previous conversation history:\n",
            "\n",
            "\n",
            "Question: Who's Thrall's father?\n",
            "Thought: Durotan is Thrall's father\n",
            "Action: Vector Search\n",
            "Action Input: father of thrall\n",
            "Observation:  The father of Thrall is Durotan. Durotan named the land Durotar for his father.\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Durotan is Thrall's father\n",
            "Final Answer: Durotan\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Durotan'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title [NOT USED] langchain RetrievalQA demo\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
        "from langchain.llms import VertexAI\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "LLM_MODEL = \"text-bison@latest\" #@param {type: \"string\"}\n",
        "MAX_OUTPUT_TOKENS = 512 #@param {type: \"integer\"}\n",
        "TEMPERATURE = 0.2 #@param {type: \"number\"}\n",
        "TOP_P = 0.8 #@param {type: \"number\"}\n",
        "TOP_K = 40 #@param {type: \"number\"}\n",
        "VERBOSE = True #@param {type: \"boolean\"}\n",
        "\n",
        "llm_palm = VertexAI(model_name=LLM_MODEL,\n",
        "      max_output_tokens=MAX_OUTPUT_TOKENS,\n",
        "      temperature=TEMPERATURE,\n",
        "      top_p=TOP_P,\n",
        "      top_k=TOP_K,\n",
        "      verbose=VERBOSE)\n",
        "\n",
        "state_of_union = RetrievalQA.from_chain_type(llm=llm_palm, chain_type=\"refine\", retriever=me_instance.as_retriever(), verbose=True, return_source_documents=False)\n",
        "tools = [\n",
        "          Tool(\n",
        "          name=\"Vector Search\",\n",
        "          func=state_of_union.run,\n",
        "          description=\"search for a query\",\n",
        "          ),\n",
        "]\n",
        "PREFIX = \"\"\"Answer the following questions as best you can. The answer should be as detailed as possible. You have access to the following tools:\"\"\"\n",
        "FORMAT_INSTRUCTIONS = \"\"\"Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times. )\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question. The final answer should be very detailed.\"\"\"\n",
        "SUFFIX = \"\"\"Begin!\n",
        "\n",
        "Previous conversation history:\n",
        "{chat_history}\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    PREFIX,\n",
        "    SUFFIX,\n",
        "    FORMAT_INSTRUCTIONS,\n",
        "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm_chain = LLMChain(llm=llm_palm, prompt=prompt, verbose=True)\n",
        "\n",
        "zs_agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
        "zs_agent_instance = AgentExecutor.from_agent_and_tools(\n",
        "    agent=zs_agent, tools=tools, verbose=True, memory=memory\n",
        ")\n",
        "COMPLEX_QUERY = \"Who's Thrall's father?\"\n",
        "zs_agent_instance.run(COMPLEX_QUERY)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IM5TWeYIBatH",
        "L2Gx7aXVBdvG",
        "eUMA6IjzBxWr",
        "Z-AU4IOwCXky",
        "BbQ9pVsCEQHF",
        "GVjqayTbHI6D"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
